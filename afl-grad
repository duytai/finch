#!/usr/bin/env python3

import time
import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn._reduction as _Reduction
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
from tqdm import tqdm

class Net(nn.Module):
    def __init__(self, in_size, out_size):
        super(Net, self).__init__()
        layer_size = [in_size, 4096, out_size]
        self.fc1 = nn.Linear(layer_size[0], layer_size[1])
        self.fc2 = nn.Linear(layer_size[1], layer_size[2])

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        return x

max_u32 = (1 << 32) - 1
gpu = 0
batch_size = 12

out_dir = "%s/%s" % (os.getcwd(), sys.argv[1])
bitmap_file = '%s/pareto/bitmap' % out_dir

if os.path.exists(bitmap_file):
    in_files = []
    label_files = []
    loc_files = []
    sign_files = []
    in_sizes = []

    pareto_bitmap = np.frombuffer(open(bitmap_file, 'rb').read(), dtype=np.uint32)
    miss_edges = np.where(pareto_bitmap != 0xffffffff)[0]

    for q in np.unique(pareto_bitmap[pareto_bitmap != 0xffffffff]):
        in_file = '%s/queue/id_%06u' %(out_dir, q)
        label_file = '%s/pareto/id_%06u' % (out_dir, q)
        loc_file = '%s/pareto/id_%06u.l' % (out_dir, q)
        sign_file = '%s/pareto/id_%06u.s' % (out_dir, q)
        if os.path.getsize(in_file) > 10000: continue
        in_files.append(in_file)
        loc_files.append(loc_file)
        label_files.append(label_file)
        sign_files.append(sign_file)
        in_sizes.append(os.path.getsize(in_file))

    inputs = np.zeros((len(in_files), max(in_sizes)))
    for idx, (in_file, in_size) in enumerate(zip(in_files, in_sizes)):
        inputs[idx][:in_size] = np.frombuffer(open(in_file, 'rb').read(), dtype=np.uint8)

    labels = np.zeros((len(in_files), miss_edges.size))
    for idx, label_file in enumerate(label_files):
        labels[idx] = np.frombuffer(open(label_file, 'rb').read(), dtype=np.uint32)[miss_edges]

    in_norms = inputs / 255.0
    loss_norms = labels / max_u32

    print("> Network %s => %s" %(inputs.shape, labels.shape));
    if torch.cuda.is_available():
        device = torch.device('cuda:%s' % gpu)
    else:
        device = torch.device('cpu')
    net = Net(in_norms[0].size, loss_norms[0].size).double().to(device)
    loss_fn = torch.nn.BCELoss()
    optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)

    print("> Start training")
    xs = torch.tensor(in_norms, device=device)
    ys = torch.tensor(loss_norms, device=device)
    data_loader = DataLoader(TensorDataset(xs, ys), batch_size=batch_size, shuffle=True)
    for epoch in tqdm(range(10)):
        for data in data_loader:
            inputs, labels = data
            optimizer.zero_grad()
            outputs = net(inputs)
            loss = loss_fn(outputs, labels)
            loss.backward()
            optimizer.step()

    print("> Compute grad")
    xs = torch.tensor(in_norms, device=device, requires_grad=True)
    ys = torch.tensor(loss_norms, device=device)
    data_loader = DataLoader(TensorDataset(xs, ys), batch_size=batch_size, shuffle=True)
    for data in data_loader:
        inputs, labels = data
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = loss_fn(outputs, labels)
        loss.backward()

    grads = xs.grad.cpu().numpy()
    locs = np.zeros(grads.shape, dtype=np.uint32)
    signs = np.zeros(grads.shape, dtype=np.int32)

    for idx, (grad, in_size) in enumerate(zip(grads, in_sizes)):
        locs[idx][:in_size] = np.abs(grad[:in_size]).argsort()[::-1]
        signs[idx][:in_size] = np.sign(grad[locs[idx][:in_size]])

    for in_size, loc_file, sign_file, loc, sign in zip(in_sizes, loc_files, sign_files, locs, signs):
        loc[:in_size].tofile(loc_file)
        sign[:in_size].tofile(sign_file)
